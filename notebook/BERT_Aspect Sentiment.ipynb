{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47063c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chloe\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training loss: 4.401010650772232, Validation loss: 3.3731067776679993\n",
      "Epoch 2/10\n",
      "Training loss: 2.9843916194932953, Validation loss: 2.4793894290924072\n",
      "Epoch 3/10\n",
      "Training loss: 2.205920749956423, Validation loss: 2.090814153353373\n",
      "Epoch 4/10\n",
      "Training loss: 1.7024906293765918, Validation loss: 1.9599006126324336\n",
      "Epoch 5/10\n",
      "Training loss: 1.3609540253072172, Validation loss: 1.8824256360530853\n",
      "Epoch 6/10\n",
      "Training loss: 1.0705540427216538, Validation loss: 1.919942984978358\n",
      "Epoch 7/10\n",
      "Training loss: 0.8523963850897711, Validation loss: 1.9851507743199666\n",
      "Epoch 8/10\n",
      "Training loss: 0.6950080370580828, Validation loss: 1.994509259859721\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chloe\\AppData\\Local\\Temp\\ipykernel_15280\\4174556368.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_bert_sentiment_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Food:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Does not exist       0.92      0.98      0.95       266\n",
      "      Negative       0.85      0.70      0.77        50\n",
      "       Neutral       0.00      0.00      0.00         6\n",
      "      Positive       0.84      0.79      0.81        58\n",
      "\n",
      "      accuracy                           0.90       380\n",
      "     macro avg       0.65      0.62      0.63       380\n",
      "  weighted avg       0.88      0.90      0.89       380\n",
      "\n",
      "Classification report for Service:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Does not exist       0.90      0.89      0.89       172\n",
      "      Negative       0.85      0.92      0.88       132\n",
      "       Neutral       0.00      0.00      0.00         4\n",
      "      Positive       0.91      0.86      0.89        72\n",
      "\n",
      "      accuracy                           0.88       380\n",
      "     macro avg       0.67      0.67      0.67       380\n",
      "  weighted avg       0.88      0.88      0.88       380\n",
      "\n",
      "Classification report for Cleanliness:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Does not exist       0.98      0.99      0.98       346\n",
      "      Negative       0.75      0.67      0.71        18\n",
      "       Neutral       0.00      0.00      0.00         1\n",
      "      Positive       0.86      0.80      0.83        15\n",
      "\n",
      "      accuracy                           0.96       380\n",
      "     macro avg       0.65      0.61      0.63       380\n",
      "  weighted avg       0.96      0.96      0.96       380\n",
      "\n",
      "Classification report for Price:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Does not exist       0.97      1.00      0.98       364\n",
      "      Negative       0.00      0.00      0.00        10\n",
      "       Neutral       0.00      0.00      0.00         3\n",
      "      Positive       0.33      0.33      0.33         3\n",
      "\n",
      "      accuracy                           0.96       380\n",
      "     macro avg       0.33      0.33      0.33       380\n",
      "  weighted avg       0.93      0.96      0.94       380\n",
      "\n",
      "Classification report for Others:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Does not exist       0.69      0.54      0.61       148\n",
      "      Negative       0.58      0.80      0.67       110\n",
      "       Neutral       0.83      0.38      0.53        26\n",
      "      Positive       0.76      0.78      0.77        96\n",
      "\n",
      "      accuracy                           0.67       380\n",
      "     macro avg       0.71      0.63      0.64       380\n",
      "  weighted avg       0.68      0.67      0.66       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from Excel\n",
    "data = pd.read_excel('cleaned_data.xlsx')\n",
    "\n",
    "# Define the dataset class\n",
    "class McDonaldsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.data.iloc[index]['review']\n",
    "        # Get the sentiment for each aspect (0, 1, 2, 3)\n",
    "        targets = self.data.iloc[index][['Food', 'Service', 'Cleanliness', 'Price', 'Others']].values.astype(int)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(targets, dtype=torch.long)  # Change to long for multi-class classification\n",
    "        }\n",
    "\n",
    "# Define the model class\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels_per_aspect):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        # A separate classifier head for each aspect (each with 4 possible classes: 0, 1, 2, 3)\n",
    "        self.food_out = nn.Linear(self.bert.config.hidden_size, num_labels_per_aspect)\n",
    "        self.service_out = nn.Linear(self.bert.config.hidden_size, num_labels_per_aspect)\n",
    "        self.cleanliness_out = nn.Linear(self.bert.config.hidden_size, num_labels_per_aspect)\n",
    "        self.price_out = nn.Linear(self.bert.config.hidden_size, num_labels_per_aspect)\n",
    "        self.others_out = nn.Linear(self.bert.config.hidden_size, num_labels_per_aspect)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        # Generate separate outputs for each aspect\n",
    "        food_output = self.food_out(output)\n",
    "        service_output = self.service_out(output)\n",
    "        cleanliness_output = self.cleanliness_out(output)\n",
    "        price_output = self.price_out(output)\n",
    "        others_output = self.others_out(output)\n",
    "        return food_output, service_output, cleanliness_output, price_output, others_output\n",
    "\n",
    "# Create a function for training the model\n",
    "def train_model(model, data_loader, loss_fn, optimizer, device):\n",
    "    model = model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        loss = sum([loss_fn(o, t) for o, t in zip(outputs, torch.unbind(targets, dim=1))])\n",
    "        \n",
    "        losses += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses / len(data_loader)\n",
    "\n",
    "# Create a function for evaluating the model\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model = model.eval()\n",
    "    losses = 0\n",
    "    predictions = []\n",
    "    targets_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            loss = sum([loss_fn(o, t) for o, t in zip(outputs, torch.unbind(targets, dim=1))])\n",
    "            \n",
    "            losses += loss.item()\n",
    "\n",
    "            preds = [torch.argmax(o, dim=1) for o in outputs]\n",
    "            predictions.append(torch.stack(preds, dim=1).cpu().numpy())\n",
    "            targets_all.append(targets.cpu().numpy())\n",
    "\n",
    "    return losses / len(data_loader), np.vstack(predictions), np.vstack(targets_all)\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "PATIENCE = 3  # Early stopping patience\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# Split dataset into train (70%), validation (15%), and test (15%)\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = McDonaldsDataset(train_data, tokenizer, MAX_LEN)\n",
    "val_dataset = McDonaldsDataset(val_data, tokenizer, MAX_LEN)\n",
    "test_dataset = McDonaldsDataset(test_data, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentimentClassifier(BERT_MODEL_NAME, num_labels_per_aspect=4)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)  # Use CrossEntropyLoss for multi-class classification\n",
    "\n",
    "# Early stopping setup\n",
    "best_loss = np.inf\n",
    "early_stop_count = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "\n",
    "    train_loss = train_model(model, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss, _, _ = eval_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "    print(f'Training loss: {train_loss}, Validation loss: {val_loss}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_bert_sentiment_model.pt')\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        if early_stop_count >= PATIENCE:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_bert_sentiment_model.pt'))\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, predictions, true_labels = eval_model(model, test_loader, loss_fn, device)\n",
    "\n",
    "# Classification report\n",
    "target_names = ['Does not exist', 'Negative', 'Neutral', 'Positive']\n",
    "for i, aspect in enumerate(['Food', 'Service', 'Cleanliness', 'Price', 'Others']):\n",
    "    print(f\"Classification report for {aspect}:\")\n",
    "    print(classification_report(true_labels[:, i], predictions[:, i], target_names=target_names, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49595265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5877d447",
   "metadata": {},
   "source": [
    "**1. Data Preparation**\n",
    "\n",
    "The data is loaded from an Excel file (cleaned_data.xlsx) containing customer reviews and their sentiment labels for aspects like Food, Service, Cleanliness, Price, and Others.  \n",
    "\n",
    "Example: A review might be: \"Food seems to be getting worse. Tables dirty. Sketchy crowd loitering within the store.\" with labels:  \n",
    "Food: 1 (Negative)  \n",
    "Service: 0 (Does not exist)  \n",
    "Cleanliness: 1 (Negative)  \n",
    "Price: 0 (Does not exist)  \n",
    "Others: 1 (Negative)  \n",
    "\n",
    "Tokenization: The BERT tokenizer encodes each review by converting it into input IDs and attention masks to be fed into the BERT model. For instance, the above review is converted into tokens, padded/truncated to a maximum length (MAX_LEN), and fed into the model.  \n",
    "\n",
    "**2. Dataset Class Creation**  \n",
    "\n",
    "The aspect sentiment targets are represented as multi-class labels (0 = Does not exist, 1 = Negative, 2 = Neutral, 3 = Positive).  \n",
    "Example: For the above review, the sentiment targets would be [1, 0, 1, 0, 1] corresponding to Food, Service, Cleanliness, Price, and Others, respectively.   \n",
    "\n",
    "**3. Model Definition**  \n",
    "Pre-trained BERT Model: The BERT model processes the tokenized input and generates a pooled output, which is a fixed-size representation of the review (sentence embedding).  \n",
    "\n",
    "Separate Classifier Heads: For each aspect (Food, Service, Cleanliness, Price, Others), there is a separate linear classifier head. Each classifier predicts one of the four possible sentiment classes: 0 (Does not exist), 1 (Negative), 2 (Neutral), or 3 (Positive).  \n",
    "\n",
    "Example: For the input review, the model outputs predictions like:  \n",
    "Food: 1 (Negative)  \n",
    "Service: 0 (Does not exist)  \n",
    "Cleanliness: 1 (Negative)  \n",
    "Price: 0 (Does not exist)  \n",
    "Others: 1 (Negative)  \n",
    "\n",
    "**4. Training Process**  \n",
    "Forward Pass: The review is passed through BERT, which generates a pooled output, then each classifier head generates predictions for each aspect.\n",
    "Loss Calculation: The model compares its predicted sentiment for each aspect with the true labels and calculates the loss using CrossEntropyLoss. This loss is used to adjust the model’s parameters.  \n",
    "Optimization: The optimizer (AdamW) updates the model parameters after each batch to minimize the loss.  \n",
    "\n",
    "**5. Early Stopping**  \n",
    "Early Stopping Mechanism: After each training epoch, the model’s performance is validated. If the validation loss does not improve for several epochs (PATIENCE), the training stops early to avoid overfitting.  \n",
    "\n",
    "**6. Model Evaluation**  \n",
    "After training, the model is evaluated on the test set, where it generates predictions for each aspect’s sentiment (Food, Service, Cleanliness, Price, Others).  \n",
    "Classification Report: A classification report is generated for each aspect, displaying precision, recall, and F1-score for all sentiment classes (Does not exist, Negative, Neutral, Positive).  \n",
    "\n",
    "Example: For the test review \"Food was slow. Manager was rude.\", the model might predict:  \n",
    "Food: 1 (Negative)  \n",
    "Service: 1 (Negative)  \n",
    "Cleanliness: 0 (Does not exist)  \n",
    "Price: 0 (Does not exist)  \n",
    "Others: 0 (Does not exist)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3ffa8",
   "metadata": {},
   "source": [
    "Example:  \n",
    "The following review: \"The food was great, but the service was slow. The place was clean, and the price was reasonable.\"  \n",
    "\n",
    "Input (X):  \n",
    "X is the review text, i.e., the entire sentence:  \n",
    "\"The food was great, but the service was slow. The place was clean, and the price was reasonable.\"\n",
    "Output (Y):  \n",
    "Y represents the sentiment for each aspect (Food, Service, Cleanliness, Price, Others), and each aspect can have one of the four sentiment labels:  \n",
    "0: Does not exist  \n",
    "1: Negative  \n",
    "2: Neutral  \n",
    "3: Positive  \n",
    "\n",
    "For this review, the output (Y) would look like this:  \n",
    "\n",
    "Food: 3 (Positive)  \n",
    "Service: 1 (Negative)  \n",
    "Cleanliness: 3 (Positive)  \n",
    "Price: 3 (Positive)  \n",
    "Others: 0 (Does not exist)  \n",
    "So, Y would be: [3, 1, 3, 3, 0], representing the sentiment for each aspect.  \n",
    "\n",
    "The BERT model processes the entire review (X) and generates an embedding (a vector representation) for the whole review.  \n",
    "Then, there are five classification heads (one for each aspect: Food, Service, Cleanliness, Price, and Others).  \n",
    "Each head takes the BERT output and predicts the sentiment (Y) for its corresponding aspect.  \n",
    "For Food, the model predicts 3 (Positive), for Service, it predicts 1 (Negative), and so on.  \n",
    "As such, the model is able to simultaneously predict sentiments for all five aspects from one review.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
